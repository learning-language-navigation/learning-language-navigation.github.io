<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description"
        content="A family of general navigation models that generalize across environments and robots, and can be readily adapted to downstream tasks.">
    <meta name="keywords" content="GNM, ViNT, NoMaD, robotics, foundation model">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>LeLaN: Learning Language-conditioned Navigation Policy</title>

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZYH3N96LN5"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'G-ZYH3N96LN5');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/slick.css">
    <link rel="stylesheet" href="./static/css/slick-theme.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/favicon.svg">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/slick.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>

<body>

    <nav class="navbar" role="navigation" aria-label="main navigation">
        <div class="navbar-brand">
            <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
            </a>
        </div>
        <div class="navbar-menu">
            <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
                <div class="navbar-item has-dropdown is-hoverable">
                    <a class="navbar-link">
                        More Research
                    </a>
                    <div class="navbar-dropdown">
                        <a class="navbar-item" href="https://svl.stanford.edu/projects/dvmpc/">
                            DVMPC: Deep Visual MPC-Policy Learning for Navigation
                        </a>
                        <a class="navbar-item" href="https://sites.google.com/view/exaug-nav">
                            ExAug: Robot-conditioned Navigation Policies via Geometric Experience Augmentation
                        </a>
                        <a class="navbar-item" href="https://sites.google.com/view/drive-any-robot">
                            GNM: A General Navigation Model to Drive Any Robot
                        </a>                        
                        <a class="navbar-item" href="https://general-navigation-models.github.io/vint/index.html">
                            ViNT: A Foundation Model for Visual Navigation
                        </a>
                        <a class="navbar-item" href="https://sites.google.com/view/sacson-review/home">
                            SACSoN: Scalable Autonomous Control for Social Navigation
                        </a>
                        <a class="navbar-item" href="https://sites.google.com/view/selfi-rl/">
                            SELFI: Autonomous Self-improvement with Reinforcement Learning for Social Navigation
                        </a>
                    </div>
                </div>
            </div>

        </div>
    </nav>
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-2 publication-title">LeLaN: Learning A Language-conditioned Navigation Policy from In-the-Wild Video</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://sites.google.com/view/noriaki-hirose/">Noriaki Hirose</a>,
                            </span>
                            <span class="author-block">
                                <a href="">Catherine Glossop</a>,
                            </span>
                            <span class="author-block">
                                <a href="http://ajaysridhar.com/">Ajay Sridhar</a>,
                            </span><br>
                            <span class="author-block">
                                <a href="https://cs.berkeley.edu/~shah">Dhruv Shah</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://www.oiermees.com/">Oier Mees</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://cs.berkeley.edu/~svlevine">Sergey Levine</a>
                            </span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- Talk Link. -->
                                <span class="link-block">
                                    <a href="https://youtu.be/Bf30cs5MU1I"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-youtube"></i>
                                        </span>
                                        <span>YouTube</span>
                                    </a>
                                </span>
                                <!-- Code Link. -->
                                <span class="link-block">
                                    <a href="https://github.com/NHirose/learning-language-navigation.git"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github-alt"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span>
                                <!-- Dataset Link. -->
                                <span class="link-block">
                                    <a href="https://github.com/robodhruv/visualnav-transformer#data-wrangling"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-database"></i> </span>
                                        <span>Data</span>
                                    </a>
                                </span>

                                <!-- BibTex -->
                                <span class="link-block">
                                    <a href="./static/lelan.bib"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-quote-left"></i> </span>
                                        <span>BibTex</span>
                                    </a>
                                </span>

                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="hero teaser">
        <div class="container is-max-desktop is-centered has-text-justified is-size-5">
            <div class="hero-body">
                <video id="teaser" disableRemotePlayback autoplay muted loop playsinline fetchpriority="high"
                    poster="./static/images/web_pull.jpeg">
                    <source src="./static/videos/web_pull.mp4" type="video/mp4">
                </video>
            </div>
        </div>
    </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
The world is filled with a wide variety of objects, ranging from teapots and wine glasses in households, to park benches and traffic cones outdoors. For robots to be useful, they need the ability to find arbitrary objects described by people. In this project, we present LeLaN, a novel approach that consumes unlabeled, action-free egocentric data to learn scalable, language-conditioned object navigation. Our framework leverages the semantic knowledge of large vision and language models to label in-the-wild data from a variety of indoor and outdoor environments. These annotations include diverse instructions that capture a wide range of objects with varied granularity and noise in their descriptions. We apply LeLaN to label over 130 hours of data collected in real world indoor and outdoor environments, including robot observations, YouTube video tours, and human-collected walking data. Extensive experiments with over 1000 trials carried out in the real world show that LeLaN enables training a policy from unlabeled action-free videos that outperforms state-of-the-art methods on the challenging zero-shot language-conditioned object navigation tasks while being capable of inference at 4 times their speed on edge compute.

            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Video</h2>
          <div class="publication-video">
            <video autoplay controls muted loop playsinline width="100%">
              <source src="static/videos/overview.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Approach</h2>
          <div class="content has-text-justified has-text-centered">
            <h3 class="title is-4">Data annotation</h3>
            <p>
              The key idea behind building a VLMap is to fuse pretrained
              visual-language features into a geometrical reconstruction of the scene. This can be
              done by computing dense pixel-level embeddings from an existing
              visual-language model (over the RGB-D video feed of the robot) and
              back-projecting them onto the 3D surface of the environment (captured
              from depth data used for reconstruction with visual odometry). Finally,
              we generate the top-down scene representation by storing the visual-language
              features of each image pixel in the corresponding grid map pixel location.
            </p>
            <img src="./static/images/system_overview.jpg" />            
            <h3 class="title is-4">Training dataset</h3>
            <p>
              We encode the open-vocabulary landmark names ("chair", "green plant", "table" etc.)
              with the text encoder in the Visual Language Model. Then we align the landmark names
              with the pixels in the VLMap by computing the cosine similarity between their embeddings.
              We get the mask of each landmark type with the argmax operation on the similarity score.
            </p>
            <h4 class="title is-5"> &#x2022; YouTube Tour Dataset</h4>            
            <video autoplay muted loop playsinline width="100%">
              <source src="static/videos/map_youtube.mp4" type="video/mp4">
            </video>     
            <h4 class="title is-5"> &#x2022; Human-walking Dataset</h4>            
            <video autoplay muted loop playsinline width="100%">
              <source src="static/videos/map_human_walking.mp4" type="video/mp4">
            </video>                        
            <h3 class="title is-4">Policy learning</h3>
            <p>
              We generate the navigation policies in the form of executable code with the help of Large Language Models.
              By providing a few examples in the prompt, we exploit GPT-3 to parse language
              instructions into a string of executable code, expressing functions
              or logic structures (if/else statements, for/while loops) and parameterizing API calls
              (e.g., robot.move_to(target_name) or robot.turn(degrees)).
            </p>
          </div>
        </div>
      </div>

      <section class="section">
        <div class="container is-max-desktop">

          <div class="columns is-centered">


            <!-- Animation. -->
            <div class="columns is-centered">
              <div class="column is-full-width">
                <h2 class="title is-3">Experiment</h2>

                <!-- Interpolating. -->
                <h3 class="title is-4">Diverse Language Instructions</h3>
                <div class="content has-text-justified">
                  <p>
                    With open-vocabulary landmark indexing, VLMaps enables long-horizon spatial goal navigation with
                    natural language instructions
                  </p>
                </div>
          </div>
      </section>

    <!-- Results Carousel -->
    <section class="hero is-light is-small">
        <div class="hero-body">
            <div class="container">
                <div id="results-carousel" class="carousel results-carousel">
                    <div class="item item-bww1">
                        <video poster="" id="bww1" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/whiteboard.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-tello">
                        <video poster="" id="tello" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/stairs.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-go1-outside">
                        <video poster="" id="go1-outside" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/person_sitting.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-soda3-left">
                        <video poster="" id="soda3-left" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/orange_vertical.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-rfs">
                        <video poster="" id="rfs" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/orange_bins.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-go1-inside-1">
                        <video poster="" id="go1-inside-1" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/glass_showcase.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-viewpoints">
                        <video poster="" id="viewpoints" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/fridge.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-go1-inside-2">
                        <video poster="" id="go1-inside-2" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/bust.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-soda3-right">
                        <video poster="" id="soda3-right" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/brick_stairs.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-soda3-right">
                        <video poster="" id="soda3-right" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/coffee_machine.mp4" type="video/mp4">
                        </video>
                    </div>                    
                </div>
            </div>            
        </div>
    </section>

      <section class="section">
        <div class="container is-max-desktop">

          <div class="columns is-centered">


            <!-- Animation. -->
            <div class="columns is-centered">
              <div class="column is-full-width">
                <!-- Interpolating. -->
                <h3 class="title is-4">Robustness for Noisy Instructions</h3>
                <div class="content has-text-justified">
                  <p>
                    With open-vocabulary landmark indexing, VLMaps enables long-horizon spatial goal navigation with
                    natural language instructions
                  </p>
                </div>
            <center>    
            <video autoplay muted loop playsinline width="80%">
              <source src="static/videos/robust_perform.mp4" type="video/mp4">
            </video>             
            </center>  
          </div>
      </section>

      <section class="section">
        <div class="container is-max-desktop">

          <div class="columns is-centered">


            <!-- Animation. -->
            <div class="columns is-centered">
              <div class="column is-full-width">
                <br />
                <!--/ Interpolating. -->

                <!-- Re-rendering. -->
                <h3 class="title is-4">Following Pedestrian</h3>
                <div class="content has-text-justified">
                  <p>
                    A VLMap can be shared among different robots and enables generation of obstacle maps for different
                    embodiments on-the-fly to improve navigation efficiency. For example, a LoCoBot (ground robot) has
                    to avoid sofa, tables, chairs and so on during planning while a drone can ignore them. Experiments
                    below show how a single VLMap representation in each scene can adapt to different embodiments
                    (by generating customized obstacle maps) and improve navigation efficiency.
                  </p>
                </div>
          </div>
      </section>

    <!-- Results Carousel -->
    <section class="hero is-light is-small">
        <div class="hero-body">
            <div class="container">
                <div id="results-carousel" class="carousel results-carousel">
                    <div class="item item-bww1">
                        <video poster="" id="bww1" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/dynamic_1.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-tello">
                        <video poster="" id="tello" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/dynamic_2.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-go1-outside">
                        <video poster="" id="go1-outside" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/dynamic_3.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-go1-outside">
                        <video poster="" id="go1-outside" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/dynamic_4.mp4" type="video/mp4">
                        </video>
                    </div>                    
                </div>
            </div>
        </div>
    </section>
      
      <section class="section">
        <div class="container is-max-desktop">

          <div class="columns is-centered">


            <!-- Animation. -->
            <div class="columns is-centered">
              <div class="column is-full-width">
                <br />
                <!--/ Interpolating. -->

                <!-- Re-rendering. -->
                <h3 class="title is-4">Cross Embodiment Navigation</h3>
                <div class="content has-text-justified">
                  <p>
                    A VLMap can be shared among different robots and enables generation of obstacle maps for different
                    embodiments on-the-fly to improve navigation efficiency. For example, a LoCoBot (ground robot) has
                    to avoid sofa, tables, chairs and so on during planning while a drone can ignore them. Experiments
                    below show how a single VLMap representation in each scene can adapt to different embodiments
                    (by generating customized obstacle maps) and improve navigation efficiency.
                  </p>
                </div>
          </div>
      </section>

    <!-- Results Carousel -->
    <section class="hero is-light is-small">
        <div class="hero-body">
            <div class="container">
                <div id="results-carousel" class="carousel results-carousel">
                    <div class="item item-bww1">
                        <video poster="" id="go1_1" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/go1_1.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-tello">
                        <video poster="" id="d435_1" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/d435_1.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-go1-outside">
                        <video poster="" id="height_1" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/height_1.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-go1-outside">
                        <video poster="" id="ricoh_1" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/ricoh_1.mp4" type="video/mp4">
                        </video>
                    </div>                    
                    <div class="item item-soda3-left">
                        <video poster="" id="d435_2" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/d435_2.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-rfs">
                        <video poster="" id="fisheye_1" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/fisheye_1.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-go1-outside">
                        <video poster="" id="ricoh_2" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/ricoh_2.mp4" type="video/mp4">
                        </video>
                    </div>                       
                    <div class="item item-go1-inside-1">
                        <video poster="" id="go1_2" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/go1_2.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-viewpoints">
                        <video poster="" id="d435_3" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/d435_3.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-go1-inside-2">
                        <video poster="" id="height_2" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/height_2.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-soda3-right">
                        <video poster="" id="fisheye_2" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/fisheye_2.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-go1-outside">
                        <video poster="" id="ricoh_3" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/ricoh_3.mp4" type="video/mp4">
                        </video>
                    </div>                       
                    <div class="item item-go1-inside-1">
                        <video poster="" id="go1_3" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/go1_3.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-viewpoints">
                        <video poster="" id="d435_4" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/d435_4.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-go1-inside-2">
                        <video poster="" id="height_3" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/height_3.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-go1-outside">
                        <video poster="" id="ricoh_4" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/ricoh_4.mp4" type="video/mp4">
                        </video>
                    </div>                       
                    <div class="item item-soda3-right">
                        <video poster="" id="d435_5" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/d435_5.mp4" type="video/mp4">
                        </video>
                    </div>                    
                </div>
            </div>
        </div>
    </section>

      <section class="section">
        <div class="container is-max-desktop">

          <div class="columns is-centered">


            <!-- Animation. -->
            <div class="columns is-centered">
              <div class="column is-full-width">
                <!-- Interpolating. -->
                <h3 class="title is-4">Collision Avoidance</h3>
                <div class="content has-text-justified">
                  <p>
                    With open-vocabulary landmark indexing, VLMaps enables long-horizon spatial goal navigation with
                    natural language instructions
                  </p>
                </div>
            <center>    
            <video autoplay muted loop playsinline width="100%">
              <source src="static/videos/collision.mp4" type="video/mp4">
            </video>             
            </center>  
          </div>
      </section>

      <section class="section">
        <div class="container is-max-desktop">

          <div class="columns is-centered">


            <!-- Animation. -->
            <div class="columns is-centered">
              <div class="column is-full-width">
                <br />
                <!--/ Interpolating. -->

                <!-- Re-rendering. -->
                <h3 class="title is-4">Multiple Objects</h3>
                <div class="content has-text-justified">
                  <p>
                    A VLMap can be shared among different robots and enables generation of obstacle maps for different
                    embodiments on-the-fly to improve navigation efficiency. For example, a LoCoBot (ground robot) has
                    to avoid sofa, tables, chairs and so on during planning while a drone can ignore them. Experiments
                    below show how a single VLMap representation in each scene can adapt to different embodiments
                    (by generating customized obstacle maps) and improve navigation efficiency.
                  </p>
                </div>
          </div>
      </section>

    <!-- Results Carousel -->
    <section class="hero is-light is-small">
        <div class="hero-body">
            <div class="container">
                <div id="results-carousel" class="carousel results-carousel">
                    <div class="item item-bww1">
                        <video poster="" id="bww1" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/multi_1.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-tello">
                        <video poster="" id="tello" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/multi_2.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-go1-outside">
                        <video poster="" id="go1-outside" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/multi_3.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="item item-go1-outside">
                        <video poster="" id="go1-outside" autoplay controls muted loop playsinline height="100%">
                            <source src="./static/videos/multi_4.mp4" type="video/mp4">
                        </video>
                    </div>                    
                </div>
            </div>
        </div>
    </section>

    <br>
    <center class="is-size-10">
      The website (<a href="https://github.com/learning-language-navigation/learning-language-navigation.github.io.git">source code</a>) design was adapted from <a href="https://nerfies.github.io" class="external-link"><span
                class="dnerf">Nerfies</span></a>.
    </center>
    <br>
</body>

</html>
